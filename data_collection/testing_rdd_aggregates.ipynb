{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from google.cloud import storage\n",
    "from mongodb import *\n",
    "from pyspark.sql import Row, SparkSession\n",
    "import pyspark\n",
    "from user_definition import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/26 16:19:50 WARN Utils: Your hostname, Ajayeswars-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.0.0.11 instead (on interface en0)\n",
      "23/02/26 16:19:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/26 16:19:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "\n",
    "class MongoDBCollection:\n",
    "    def __init__(self,\n",
    "                 username,\n",
    "                 password,\n",
    "                 ip_address,\n",
    "                 database_name,\n",
    "                 collection_name):\n",
    "        '''\n",
    "        Using ip_address, database_name, collection_name,\n",
    "        initiate the instance's attributes including ip_address,\n",
    "        database_name, collection_name, client, db and collection.\n",
    "\n",
    "        For pymongo, see more details in the following.\n",
    "        https://pymongo.readthedocs.io\n",
    "        '''\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        \n",
    "        self.ip_address = ip_address\n",
    "        self.database_name = database_name\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        self.client = pymongo.MongoClient(f\"mongodb+srv://{username}:{password}@{ip_address}\")\n",
    "        self.db = self.client[database_name]\n",
    "        self.collection = self.db[collection_name]\n",
    "        \n",
    "        self.connection_string = f\"mongodb+srv://{username}:{password}@{ip_address}/{database_name}.{collection_name}\"\n",
    "\n",
    "    def return_db(self):\n",
    "        '''\n",
    "        Return db which is the database in the client\n",
    "        '''\n",
    "        return self.db\n",
    "\n",
    "    def return_collection(self):\n",
    "        '''\n",
    "        Return db which belongs to the db.\n",
    "        '''\n",
    "        return self.collection\n",
    "\n",
    "    def return_num_docs(self, query):\n",
    "        '''\n",
    "        Return the number of documents satisfying the given query.\n",
    "        '''\n",
    "        return self.collection.count_documents(query)\n",
    "\n",
    "    def drop_collection(self):\n",
    "        '''\n",
    "        Drop the collection\n",
    "        '''\n",
    "        return self.collection.drop()\n",
    "\n",
    "    def find(self, query, projection):\n",
    "        '''\n",
    "        Return an iteratatable using query and projection.\n",
    "        '''\n",
    "        for item in self.collection.find(query, projection):\n",
    "            yield item\n",
    "\n",
    "    def insert_one(self, doc):\n",
    "        '''\n",
    "        Insert the given document\n",
    "        '''\n",
    "        self.collection.insert_one(doc)\n",
    "\n",
    "    def insert_many(self, docs):\n",
    "        '''\n",
    "        Insert the given documents\n",
    "        '''\n",
    "        self.collection.insert_many(docs)\n",
    "\n",
    "    def update_many(self, filter, update):\n",
    "        '''\n",
    "        Update documents satisfying filter with update.\n",
    "        Both filter and update are dictionaries.\n",
    "        '''\n",
    "        self.collection.update_many(filter, update)\n",
    "\n",
    "    def find_id_for_cad_number(self, cad_number):\n",
    "        '''\n",
    "        Return _id for the given cad_numer\n",
    "        '''\n",
    "        return self.find({\"cad_number\": f\"{cad_number}\"}, {\"_id\": 1})\n",
    "\n",
    "    def remove_record_in_weather(self, cad_number, val):\n",
    "        '''\n",
    "        Remove items in the weather array where the \"value\" is val.\n",
    "        '''\n",
    "        return self.update_many({\"cad_number\": f\"{cad_number}\"},\n",
    "                                {'$pull': {\"weather\": {\"value\": val}}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_json(service_account_key_file,\n",
    "                bucket_name,\n",
    "                blob_name):\n",
    "    storage_client = storage.Client.from_service_account_json('/Users/ajay/Downloads/dds/ResumeMatcher/data_collection/msds697-group6-key.json')\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    json_str = blob.download_as_string().decode(\"utf8\")\n",
    "    json_data = json.loads(json_str)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday = '2023-02-22'\n",
    "jobs_data_path = f'{yesterday}/jobs.json'\n",
    "jobs_json = return_json(service_account_key_file,\n",
    "                               bucket_name,\n",
    "                               jobs_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SmMip7HqlkkAAAAAAAAAAA==', '0WBOJan3AZEAAAAAAAAAAA==', 'OMY_1fMqjVoAAAAAAAAAAA==', 'uYyH_F01ClwAAAAAAAAAAA==', 'RzFaYg_dytcAAAAAAAAAAA==', 'PLjfvqTM_cEAAAAAAAAAAA==', 'vBNGSQeap8cAAAAAAAAAAA==', 'uEOSwQHXdOkAAAAAAAAAAA==', 'TMX7lnb35pwAAAAAAAAAAA==', 'SKF6g2gJELEAAAAAAAAAAA==', 'Y1gXC0-lnBYAAAAAAAAAAA==', 'BbDqzJKFKgcAAAAAAAAAAA==', 'y8az4xDo9sgAAAAAAAAAAA==', 'PGc8ikPq4EgAAAAAAAAAAA==', 'XbdDbUXF-ZAAAAAAAAAAAA==', 'Gf6t0O9mWFkAAAAAAAAAAA==', '5hBtp2DVuX0AAAAAAAAAAA==', 'dnn7rKLAtYsAAAAAAAAAAA==', 'z2vZPYOx9rsAAAAAAAAAAA=='])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_rdd = sc.parallelize([jobs_json])\n",
    "jobs_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fields(ip_json):\n",
    "    keys = list(ip_json.keys())\n",
    "    final_list = []\n",
    "    for k in keys:\n",
    "        job_j = ip_json[k][0]\n",
    "        filtered_dict = {}\n",
    "        filtered_dict['job_id'] = job_j['job_id']\n",
    "        filtered_dict['job_title'] = job_j['job_title']\n",
    "        filtered_dict['job_description'] = job_j['job_description']\n",
    "        filtered_dict['employer_name'] = job_j['employer_name']\n",
    "        filtered_dict['job_required_skills'] = job_j['job_required_skills']\n",
    "        filtered_dict['job_salary_currency'] = job_j['job_salary_currency']\n",
    "        filtered_dict['job_highlights'] = job_j['job_highlights']\n",
    "        final_list.append(filtered_dict)\n",
    "    return final_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_aggregate = jobs_rdd.map(lambda x:filter_fields(x))\n",
    "jobs_filtered_list =jobs_aggregate.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aggregate in jobs_filtered_list:\n",
    "    print(aggregate.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jobs_seo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_seo_data_path = f'{yesterday}/jobs_seo.json'\n",
    "jobs_seo_json = return_json(service_account_key_file,\n",
    "                               bucket_name,\n",
    "                               jobs_seo_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_seo_rdd = sc.parallelize([jobs_seo_json])\n",
    "jobs_seo_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fields_seo(ip_json):\n",
    "    keys = list(ip_json.keys())\n",
    "    final_list = []\n",
    "    for k in keys:\n",
    "        job_j = ip_json[k]\n",
    "        filtered_dict = {}\n",
    "        # filtered_dict['job_id'] = job_j['job_id']\n",
    "        filtered_dict['job_title'] = job_j['position']\n",
    "        filtered_dict['job_description'] = job_j['description']\n",
    "        filtered_dict['employer_name'] = job_j['employer']\n",
    "        # filtered_dict['job_required_skills'] = job_j['job_required_skills']\n",
    "        # filtered_dict['job_salary_currency'] = job_j['job_salary_currency']\n",
    "        filtered_dict['job_highlights'] = job_j['highlights']\n",
    "        final_list.append(filtered_dict)\n",
    "    return final_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_seo_aggregate = jobs_seo_rdd.map(lambda x:filter_fields_seo(x))\n",
    "jobs_seo_filtered_list =jobs_seo_aggregate.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodb = MongoDBCollection(mongo_username,\n",
    "                                mongo_password,\n",
    "                                mongo_ip_address,\n",
    "                                database_name,\n",
    "                                collection_name)\n",
    "\n",
    "for aggregate in jobs_seo_filtered_list:\n",
    "    mongodb.insert_one(aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9b70a2f53b3123c4f3f0fd213752fc17e60208f64c31180e092a880f7d56e3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
